{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82587c55-373f-45bd-bb36-9bd5e35d35d6",
   "metadata": {},
   "source": [
    "# Analyse et Prédiction de la COVID-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745cc49-75bf-4b34-a467-21328874647a",
   "metadata": {},
   "source": [
    "## Aperçu du projet\n",
    "Ce projet analyse les données officielles de la COVID-19 fournies par l’Organisation Mondiale de la Santé (OMS).  \n",
    "L’objectif est d’explorer le jeu de données, de construire des modèles de Machine Learning pour la prédiction/classification, et de fournir des informations utiles pour la prise de décision en santé publique.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90e12c-ed2f-401e-97b6-58cb29dcd90b",
   "metadata": {},
   "source": [
    "#### Courte description\n",
    "Nous avons collecté les statistiques mondiales de la COVID-19 (cas, décès, hospitalisations, vaccinations).  \n",
    "Les données ont été nettoyées et préparées, une analyse exploratoire a été réalisée, des modèles de Machine Learning (régression logistique, forêt aléatoire) ont été construits, et une classification a été effectuée pour détecter les zones à **haut risque**.  \n",
    "Enfin, des visualisations et des recommandations ont été produites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24931d37-0edb-462e-b722-f377a5f1f1cc",
   "metadata": {},
   "source": [
    "## Compréhension du problème (Business Understanding)\n",
    "- **Problème** : Les autorités de santé publique ont besoin d’anticiper les hausses potentielles de cas de COVID-19.  \n",
    "- **Objectif** : Construire un modèle prédictif capable de classifier si un pays/région est en **haut risque** d’une flambée épidémique.  \n",
    "- **Critères de succès** : Obtenir une précision d’au moins 75% et générer des insights exploitables par les décideurs.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3527d7e-a4a8-4277-af3c-a3201e6b9c4e",
   "metadata": {},
   "source": [
    "## Compréhension des données (Data Understanding)\n",
    "- **Source** : OMS – bases de données officielles COVID-19 (cas, décès, vaccination, tests).  \n",
    "- **Format** : Fichiers CSV mis à jour régulièrement.  \n",
    "- **Variables disponibles** :  \n",
    "  - Date, Pays, Cas, Décès, Hospitalisations  \n",
    "  - Taux de vaccination (% de population vaccinée)  \n",
    "  - Indicateurs de politique sanitaire (confinement, port du masque, etc.)  \n",
    "- **Observations initiales** :  \n",
    "  - Données manquantes pour certains pays.  \n",
    "  - Hétérogénéité dans la fréquence de mise à jour (quotidienne vs hebdomadaire).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bca736a-c5ac-4422-9a9b-8a1fc7516a58",
   "metadata": {},
   "source": [
    "## Préparation des données (Data Preparation)\n",
    "Étapes effectuées :  \n",
    "1. Chargement des fichiers CSV (base OMS).  \n",
    "2. Nettoyage des valeurs manquantes et doublons.  \n",
    "3. Conversion des dates au format `datetime` et création de nouvelles variables temporelles (semaine, mois, année).  \n",
    "4. Normalisation des mesures par population (cas pour 100 000 habitants, décès pour 100 000 habitants).  \n",
    "5. Création d’une variable cible **Haut_Risque (0/1)** = 1 si augmentation des cas >20% dans les 14 jours suivants.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc4128-f54e-4423-961b-b2a573a68841",
   "metadata": {},
   "source": [
    "### Librairies pour la manipulation de données\n",
    "- `pandas (pd)` : création et manipulation des DataFrames.\n",
    "- `numpy (np)` : calculs numériques et traitement de matrices/vecteurs.\n",
    "\n",
    "### Librairies pour visualisation\n",
    "- `matplotlib.pyplot (plt)` : création de graphiques personnalisables.\n",
    "- `seaborn (sns)` : rend les graphiques plus esthétiques et fournit des fonctions statistiques avancées.\n",
    "\n",
    "### Librairies pour Machine Learning\n",
    "- `train_test_split` : divise les données en ensembles d’entraînement et test.\n",
    "- `GridSearchCV` : recherche automatique des meilleurs hyperparamètres via validation croisée.\n",
    "- `TimeSeriesSplit` : pour séries temporelles, division chronologique des données.\n",
    "- `StandardScaler` : normalise les variables numériques (moyenne=0, écart-type=1).\n",
    "- `Pipeline` : permet de chaîner plusieurs étapes (ex : normalisation + modèle).\n",
    "- `LogisticRegression` : modèle de régression logistique pour classification binaire.\n",
    "- `RandomForestClassifier` : modèle de forêt aléatoire basé sur plusieurs arbres de décision.\n",
    "\n",
    "### Librairies pour évaluation des modèles\n",
    "- `accuracy_score` : proportion de prédictions correctes.\n",
    "- `precision_score` : proportion de vrais positifs parmi les prédictions positives.\n",
    "- `recall_score` : proportion de vrais positifs parmi les vrais positifs totaux.\n",
    "- `f1_score` : moyenne harmonique de précision et rappel.\n",
    "- `confusion_matrix` : matrice affichant vrais/faux positifs et négatifs.\n",
    "- `roc_auc_score` : surface sous la courbe ROC.\n",
    "- `roc_curve` : points pour tracer la courbe ROC.\n",
    "- `classification_report` : résumé complet des métriques (precision, recall, f1, support).\n",
    "\n",
    "### Gestion des avertissements\n",
    "- `warnings.filterwarnings('ignore')` : ignore les warnings éventuels, par exemple sur des paramètres dépréciés.\n",
    "\n",
    "### Affichage graphique\n",
    "- `sns.set(style=\"whitegrid\")` : style des graphiques avec fond blanc et grille discrète, pour plus de lisibilité.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a8daca-118a-45b4-89d9-a89043438380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import des librairies principales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    ")\n",
    "\n",
    "# Pour gérer les avertissements éventuels\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Affichage plus clair\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57354c15-3c05-4015-8485-6a75dd2a383a",
   "metadata": {},
   "source": [
    "#  Chargement des données COVID-19\n",
    "\n",
    "Dans cette section, nous allons **importer les données COVID-19** fournies par l’Organisation mondiale de la santé (OMS) et effectuer une première vérification pour comprendre la structure du jeu de données.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Charger les données\n",
    "\n",
    "```python\n",
    "# 2. Charger les données \n",
    "df = pd.read_csv(\"Data/WHO-COVID-19-global-daily-data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58036a5-b470-41ce-ac13-5b507dcf9632",
   "metadata": {},
   "source": [
    "# Vérification et nettoyage des données\n",
    "\n",
    "Avant de commencer l’analyse, il est essentiel de **vérifier la qualité des données** et de procéder à un **nettoyage**.  \n",
    "Cela permet d’éviter les erreurs et de rendre le jeu de données exploitable.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Vérifier les informations générales\n",
    "\n",
    "```python\n",
    "print(df.info())\n",
    "print(\"\\nValeurs manquantes par colonne :\")\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1d4ec-06c9-4262-99ad-2f977f654c56",
   "metadata": {},
   "source": [
    "#  Conversion des dates et création de variables temporelles\n",
    "\n",
    "Après avoir converti la colonne **`Date_reported`** en format `datetime`, il est possible d’extraire de nouvelles variables temporelles.  \n",
    "Ces variables facilitent les analyses **par période** (semaine, mois, année).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Création des colonnes temporelles\n",
    "\n",
    "```python\n",
    "df[\"semaine\"] = df[\"Date_reported\"].dt.isocalendar().week\n",
    "df[\"mois\"] = df[\"Date_reported\"].dt.month\n",
    "df[\"année\"] = df[\"Date_reported\"].dt.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59b0d9-5fc3-4060-b9a1-0ab4dec30530",
   "metadata": {},
   "source": [
    "#  Création de la variable cible : `Haut_Risque`\n",
    "\n",
    "Dans cette étape, nous créons une variable **cible binaire** qui permettra d’identifier les pays ou situations considérées comme étant à **haut risque** d’augmentation des cas de COVID-19.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Trier les données par pays et date\n",
    "df = df.sort_values(['Country', 'Date_reported'])\n",
    "\n",
    "# Calcul du nombre de cas 14 jours après\n",
    "df['Cas_14j'] = df.groupby('Country')['New_cases'].shift(-14)\n",
    "\n",
    "# Calcul de l'augmentation en pourcentage\n",
    "df['Augmentation'] = (df['Cas_14j'] - df['New_cases']) / df['New_cases']\n",
    "\n",
    "# Définition de la variable cible : 1 si augmentation > 20%, sinon 0\n",
    "df['Haut_Risque'] = np.where(df['Augmentation'] > 0.2, 1, 0)\n",
    "\n",
    "# Supprimer les lignes finales sans Cas_14j\n",
    "df.dropna(subset=['Cas_14j'], inplace=True)\n",
    "\n",
    "# Affichage des premières lignes\n",
    "df[['Country','Date_reported','New_cases','Cas_14j','Augmentation','Haut_Risque']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763148e-22c2-4a71-8c37-1707dce355ec",
   "metadata": {},
   "source": [
    "### Préparer les features (X) et la cible (y) pour l’entraînement des modèles\n",
    "\n",
    "Dans cette étape, nous allons définir :\n",
    "\n",
    "- **La cible (`y`)** : la variable que nous cherchons à prédire, ici `Haut_Risque`.  \n",
    "- **Les features (`X`)** : les variables explicatives que nous allons fournir aux modèles d’apprentissage automatique.\n",
    "\n",
    "---\n",
    "\n",
    "#### Code Python\n",
    "\n",
    "```python\n",
    "# Définir la cible\n",
    "y = df['Haut_Risque']\n",
    "\n",
    "# Pour l’instant, on prend des features simples (on pourra enrichir plus tard)\n",
    "X = df[['New_cases', 'New_deaths', 'semaine', 'mois', 'année']]\n",
    "\n",
    "# Vérifions les dimensions\n",
    "print(\"Shape X :\", X.shape)\n",
    "print(\"Shape y :\", y.shape)\n",
    "\n",
    "# Affichage des premières lignes de X\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae61af9-8193-4f28-b2f4-80a794cf6b56",
   "metadata": {},
   "source": [
    "### Distribution des cas et décès par région\n",
    "\n",
    "Dans cette partie, nous analysons la répartition **des cas** et **des décès** de COVID-19 par région de l’OMS (`WHO_region`).\n",
    "\n",
    "---\n",
    "\n",
    "#### Code Python\n",
    "\n",
    "```python\n",
    "# Cas et décès totaux par région\n",
    "region_cases = df.groupby('WHO_region')['New_cases'].sum().sort_values(ascending=False)\n",
    "region_deaths = df.groupby('WHO_region')['New_deaths'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Cas totaux par région\n",
    "region_cases_df = region_cases.reset_index()\n",
    "region_cases_df.columns = ['WHO_region', 'Total_Cases']\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(\n",
    "    x='WHO_region',\n",
    "    y='Total_Cases',\n",
    "    data=region_cases_df,\n",
    "    color=\"red\",    # on remplace palette par color\n",
    ")\n",
    "plt.title(\"Cas totaux par région\")\n",
    "plt.ylabel(\"Nombre de cas\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Décès totaux par région\n",
    "region_deaths_df = region_deaths.reset_index()\n",
    "region_deaths_df.columns = ['WHO_region', 'Total_Deaths']\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(\n",
    "    x='WHO_region',\n",
    "    y='Total_Deaths',\n",
    "    data=region_deaths_df,\n",
    "    color=\"blue\",   # on remplace palette par color\n",
    ")\n",
    "plt.title(\"Décès totaux par région\")\n",
    "plt.ylabel(\"Nombre de décès\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab53df2-3fc6-48d7-a161-754e18d0a8a1",
   "metadata": {},
   "source": [
    "### Visualisation des vagues épidémiques au fil du temps  \n",
    "\n",
    "Dans cette étape, on représente graphiquement l’évolution du nombre de cas et de décès dans un pays donné, afin d’identifier les vagues épidémiques.  \n",
    "Nous choisissons ici la **France** comme exemple.  \n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(14,6))\n",
    "# Exemple pour un pays spécifique, par ex. \"France\"\n",
    "country = \"France\"\n",
    "df_country = df[df['Country']==country]\n",
    "\n",
    "# Tracer les cas et décès au fil du temps\n",
    "sns.lineplot(x='Date_reported', y='New_cases', data=df_country, label='Cas')\n",
    "sns.lineplot(x='Date_reported', y='New_deaths', data=df_country, label='Décès')\n",
    "\n",
    "# Personnalisation du graphique\n",
    "plt.title(f\"Vagues épidémiques au fil du temps - {country}\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Nombre quotidien\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57af74-f32c-4b9e-9931-e708672d66a8",
   "metadata": {},
   "source": [
    "### Préparation des données pour Machine Learning  \n",
    "\n",
    "Avant d’entraîner nos modèles, nous devons préparer correctement les données :  \n",
    "- Sélectionner les variables explicatives (**features**) qui serviront de prédicteurs.  \n",
    "- Définir la variable cible (**target**) : ici, `Haut_Risque`.  \n",
    "- Normaliser les données pour que toutes les variables soient sur la même échelle.  \n",
    "- Diviser le jeu de données en ensembles d’entraînement et de test.  \n",
    "\n",
    "```python\n",
    "# Sélection des colonnes existantes\n",
    "features = ['New_cases', 'New_deaths']  # ajouter 'Taux_vaccination' si dispo\n",
    "X = df[features].fillna(0)\n",
    "y = df['Haut_Risque']\n",
    "\n",
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Séparation train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Données prêtes pour le ML\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d40ea-154f-421b-bb27-2627ec4099c6",
   "metadata": {},
   "source": [
    "### Régression Logistique  \n",
    "\n",
    "Nous utilisons ici une **pipeline** combinant la normalisation des données et un classificateur de **régression logistique**.  \n",
    "Un `GridSearchCV` permet de trouver le meilleur hyperparamètre `C` grâce à une validation croisée (cv=3).  \n",
    "\n",
    "```python\n",
    "# Pipeline + GridSearch pour Logistic Regression\n",
    "pipe_lr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42))\n",
    "])\n",
    "\n",
    "# Paramètres à tester\n",
    "param_lr = {\"clf__C\": [0.1, 1, 3]}\n",
    "\n",
    "# GridSearch avec validation croisée\n",
    "grid_lr = GridSearchCV(pipe_lr, param_lr, scoring=\"f1\", cv=3, n_jobs=-1)\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_lr = grid_lr.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(\"=== Régression Logistique ===\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Matrice de confusion\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - Régression Logistique\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6aef62-71e6-416a-a7f3-b98be00587db",
   "metadata": {},
   "source": [
    "### Modèle Random Forest simple\n",
    "\n",
    "On entraîne un modèle **Random Forest** pour prédire la variable cible `Haut_Risque` à partir des colonnes `New_cases` et `New_deaths`.\n",
    "\n",
    "```python\n",
    "# --- Modèle Random Forest simple ---\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,          # Nombre d'arbres dans la forêt\n",
    "    max_depth=None,            # Profondeur maximale des arbres\n",
    "    min_samples_split=2,       # Nombre minimum d'échantillons pour diviser un noeud\n",
    "    min_samples_leaf=1,        # Nombre minimum d'échantillons dans une feuille\n",
    "    random_state=42,\n",
    "    n_jobs=-1                  # Utilisation de tous les cœurs pour l'entraînement\n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(\"=== Forêt Aléatoire simple ===\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Matrice de confusion\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "plt.title(\"Matrice de confusion - Forêt Aléatoire\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26efc222-c95c-4001-a25d-8c7c2461369e",
   "metadata": {},
   "source": [
    "### Exemple minimal : Random Forest avec GridSearchCV\n",
    "\n",
    "Nous allons entraîner un modèle **Random Forest** sur un petit jeu de données fictif pour prédire la variable `Haut_Risque`.\n",
    "\n",
    "```python\n",
    "# 2. Exemple de dataframe minimal\n",
    "df = pd.DataFrame({\n",
    "    \"New_cases\": [10, 20, 30, 40, 50, 60, 70, 80],\n",
    "    \"New_deaths\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    \"Haut_Risque\": [0,0,1,1,0,1,0,1]\n",
    "})\n",
    "\n",
    "# 3. Préparation des features et target\n",
    "features = [\"New_cases\", \"New_deaths\"]\n",
    "X = df[features].fillna(0)  # Remplacer les valeurs manquantes par 0\n",
    "y = df[\"Haut_Risque\"]\n",
    "\n",
    "# 4. Normalisation et séparation train/test\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 5. Définition des hyperparamètres pour GridSearch\n",
    "param_rf = {\n",
    "    \"n_estimators\": [100, 200],       # Nombre d'arbres\n",
    "    \"max_depth\": [None, 10, 20],      # Profondeur maximale\n",
    "    \"min_samples_split\": [2, 5],      # Minimum d'échantillons pour diviser un noeud\n",
    "    \"min_samples_leaf\": [1, 2]        # Minimum d'échantillons dans une feuille\n",
    "}\n",
    "\n",
    "# 6. GridSearchCV pour trouver la meilleure combinaison\n",
    "grid_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42), \n",
    "    param_rf, \n",
    "    scoring=\"f1\", \n",
    "    cv=3, \n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# 7. Meilleur modèle\n",
    "best_rf = grid_rf.best_estimator_\n",
    "print(\"Random Forest entraînée avec GridSearch\")\n",
    "print(\"Meilleurs paramètres :\", grid_rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988df5a2-8026-4fa6-a6e3-f954964ba023",
   "metadata": {},
   "source": [
    "### Prédictions et évaluation du modèle Random Forest\n",
    "\n",
    "Après avoir entraîné le modèle Random Forest avec GridSearchCV, on peut maintenant faire des prédictions et évaluer ses performances.\n",
    "\n",
    "```python\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Évaluation des performances\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matrice de confusion\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "plt.title(\"Matrice de confusion - Random Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b201bf-3ddd-4180-a9d3-ef5c9d52fed4",
   "metadata": {},
   "source": [
    "### Courbe ROC - Random Forest\n",
    "\n",
    "La courbe ROC (Receiver Operating Characteristic) permet d'évaluer la capacité du modèle à distinguer les classes positives et négatives.\n",
    "\n",
    "```python\n",
    "\n",
    "# --- Features et target ---\n",
    "X = df[[\"New_cases\", \"New_deaths\"]].fillna(0)\n",
    "y = df[\"Haut_Risque\"]\n",
    "\n",
    "# --- Train/Test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# --- Normalisation ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Entraînement Random Forest simple ---\n",
    "best_rf = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, class_weight=\"balanced\")\n",
    "best_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# --- Prédictions et probabilités ---\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "y_prob = best_rf.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "# --- Évaluation ---\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Courbe ROC ---\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\", color=\"green\")\n",
    "plt.plot([0,1],[0,1],\"k--\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"Courbe ROC - Random Forest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be593860-39f2-4534-ba88-bf81ee9e7151",
   "metadata": {},
   "source": [
    "## Recommandations commerciales basées sur l'analyse COVID-19\n",
    "*Renforcer la surveillance dans les régions à forte augmentation des cas.*\n",
    " \n",
    " *Planification proactive des mesures sanitaires (campagnes de vaccination, communication\n",
    "publique).*\n",
    "\n",
    " *Allocation des ressources (personnel, équipements, tests) en fonction des prédiction*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d107700d-efa4-4706-94da-1a828376e73a",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "Ajouter des données supplémentaires (vaccination, tests, densité population).\n",
    "\n",
    "Tester d’autres modèles pour améliorer la précision. Créer un dashboard interactif pour\n",
    "suivre en temps\n",
    "\n",
    "réel les périodes à risque. Collaborer avec les autorités sanitaires pour adapter les stratégie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc4881-98f4-495e-90cd-8e345862cee0",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "Les modèles de Machine Learning permettent de prévoir les périodes à haut risque COVID19.\n",
    "\n",
    "Ils aident les décideurs à prendre des décisions\n",
    "\n",
    "éclairées et à mieux préparer les réponses sanitai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee64bd-71a4-433e-aa8c-93c4823105b0",
   "metadata": {},
   "source": [
    "##### Email: septamalouison634@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c36cd-e6b6-46b5-a6df-112c7c20811e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
